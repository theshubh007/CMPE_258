{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning TinyLlama with Extended Context Length\n",
        "\n",
        "In this notebook, I fine-tuned the `TinyLlama-1.1B-Chat-v1.0` model using Hugging Face Transformers and PEFT (Parameter-Efficient Fine-Tuning). The goal was to extend its context length capabilities and adapt it to a specific task. Key steps include:\n",
        "\n",
        "- Installing necessary libraries such as `transformers`, `datasets`, `bitsandbytes`, and `peft`.\n",
        "- Loading the TinyLlama model with 4-bit quantization for memory-efficient training.\n",
        "- Preparing the dataset using Hugging Face's `datasets` library and formatting it appropriately.\n",
        "- Applying LoRA (Low-Rank Adaptation) using PEFT to fine-tune specific projection layers.\n",
        "- Configuring training parameters including extended context size, learning rate, and batch settings.\n",
        "- Training the model and evaluating its capability to handle longer context sequences.\n",
        "- Saving the fine-tuned model for future inference or deployment.\n",
        "\n",
        "This process showcases how to efficiently fine-tune and extend the capabilities of compact LLMs like TinyLlama using PEFT and quantized loading.\n"
      ],
      "metadata": {
        "id": "An7BVd1xRv0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJtCPnnRuLHu",
        "outputId": "f9c77b77-748f-48ea-d0e2-0dfe2d3d2394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes-cpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes-cpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate bitsandbytes sentencepiece wandb einops peft\n",
        "!pip install bitsandbytes-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Python Modules\n",
        "- Import essential libraries like `torch`, `transformers`, `datasets`, and `peft`.\n",
        "- These modules are used for model loading, dataset handling, and training.\n"
      ],
      "metadata": {
        "id": "_cfbZLDySKwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckzZTmMZujbm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v342p5rAun2Y"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "transformers.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Configuration Class\n",
        "- Create a configuration class to store hyperparameters and training settings.\n",
        "- Includes model path, sequence lengths, LoRA config, and batch sizes.\n"
      ],
      "metadata": {
        "id": "tCa387XdSNaw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRbaig7nupfK"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ORIGINAL_CTX_LENGTH = 2048  # Original context length of TinyLlama\n",
        "TARGET_CTX_LENGTH = 8192   # Target context length after extension\n",
        "BATCH_SIZE = 4\n",
        "LORA_RANK = 16\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Quantization\n",
        "- Set up 4-bit quantization using `BitsAndBytesConfig` to reduce memory usage.\n",
        "- Enables efficient model training on limited hardware.\n"
      ],
      "metadata": {
        "id": "HIOp4z6ASR_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWug4Axeupcy"
      },
      "outputs": [],
      "source": [
        "# Setup model with BF16 mixed precision\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "86fbb8d426b94d5f99ac288c9a2928a4",
            "1dff350b190d46dfbd55e97587d68c98",
            "7ec01c00fd0e40e28f9ab34d77ee2f7d",
            "dd64e84fc436489da943415ba2b34b75",
            "a61a716d755a42e7bf29b3314111f968",
            "afeb954dc5b742dd9c11c6a36a705e98",
            "e68f5af2cab84be1858b135e10f37b14",
            "24f9858452c54d918f628011f24b13a0",
            "a45e3695ffd2450da785e199cc57c15a",
            "2b6169a351c5475e87a20730bed82dfa",
            "e20171fca6644df4b2f5eccaa72590dc",
            "403c0e0a6c53454a89b253874fa2c527",
            "f300283a96e5437bad4844d9402d22c2",
            "758b82e0e61e4450bbdaa349e460a710",
            "7e66d852ebde4000b27a3825d00c0dda",
            "830a379dc80f46499810c55a381f1ac4",
            "9b75a5a4273a4b30979447fdd5cf1ebf",
            "bbd38d6774644bd4835b4645a4089926",
            "3618b470ec0b445ba6af2de6821dd4b1",
            "d1d3267b87ba4f15ab1615614f309774",
            "aaa6314d66df44dfbf76ae4556f80040",
            "8642dbbea2664684b74ff16c9bba9819",
            "7ad1704350b64508ae0e86bfaa35658c",
            "aa367f6a2b9a44aeae1490b0ae06c300",
            "86c1048e8bbe4f0b97409fb3f92d30a5",
            "0f6fd76f168c4cbfa3035df288bac031",
            "e626e36539c04713bc670879f34df2c5",
            "882ec02bfe0c4047a7566aac09d3d456",
            "0eca203a2bea4cf8befdbeac3428b1cd",
            "b7ca4f324dc0476a96d6f3f8307aed25",
            "dab5638a041e4d608a0b4677d14ab723",
            "0b40bd0d445843c6a11a9eda5e53e246",
            "1f2cdd620d5746c6aa1a62d4c8bdb86c",
            "79a1f0dbadd4496eb6de6626ffdeb4f5",
            "116765e4dc7d44d0910ce8c2bad72fb7",
            "fd713cd37c7940aba275b1d9da226b16",
            "7f94fdd2d3a44e5da8d76f3a75cecca6",
            "ce04092213c347d4bace0de39bf79cc0",
            "b7798cb3e6ae4b7eb84e11ad206516ae",
            "5eeeedb88e1c446ba9f3368be0a00b49",
            "6e001575b5f249cea6e3e4ed811262b1",
            "331e1e32849b4f799a09197dee007d0c",
            "a8127e0a493444a395ef697972e43be7",
            "6575efc94f38470f9245ea03c71e29c0"
          ]
        },
        "id": "lIwAZcsvutZB",
        "outputId": "260e59d0-5f0a-4058-e5a9-78996a2347e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86fbb8d426b94d5f99ac288c9a2928a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "403c0e0a6c53454a89b253874fa2c527",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ad1704350b64508ae0e86bfaa35658c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79a1f0dbadd4496eb6de6626ffdeb4f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Base Model\n",
        "- Load the TinyLlama base model using quantization and prepare it for LoRA training.\n",
        "- The model is wrapped for compatibility with PEFT fine-tuning.\n"
      ],
      "metadata": {
        "id": "y0pNmoN-SV9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "1e1ca2a6c9dc46e68c0b8da956c08294",
            "6a7047fa4a8042d9933fd4dada12dba4",
            "5f16fb996cd24424a1c6314a25b8db5c",
            "6ef6865fc3be4fc3935202fb81344d27",
            "68c26947b4b443bfa798619025bb8880",
            "67a39d9475824b12bc75831c9e7c2537",
            "046934a7ebcf4c96a331d3e0e5d39d36",
            "16aae69f29d5464f93369c672e716c14",
            "74bbee799a5e4c6b9527148d2c69a7d8",
            "77037f11f23e46909f5ca9db39f850ac",
            "70a86e0ecdce489aa0f6496946b2a60b",
            "b2e2145d289d4c7fac22d4ca73e0831f",
            "8e64f27655454bfd8f901eaf37a7376c",
            "f17e79788bed41f8a3b079883de5bf1e",
            "7c91aee8d667434d87a8606b0900c8fe",
            "2b470e3b5c51455fbc0d33381d951a15",
            "afb8d47d225443739f0580f755c4738d",
            "4fc6e385ba884da9951214790ac48592",
            "ae93e1a7ab0d42f7ac5d4f99c2fe0864",
            "79ec59805e8342e6a2253edf3871fc3b",
            "609a21725b0146d1935ac43745c8c864",
            "9667aa1282554589ba2fbf8cb0945e7c",
            "fc10008f48444b5db516d07b5aa07931",
            "82a8bcad40db4978acebda0163fbe7c0",
            "a7ca964c24eb431bb2cbbc688ce348d6",
            "59943538185942b1aa3283baf19e7aad",
            "673ce61123404a1aa544a1484ba5ab1d",
            "71f4e6fe7d0e4f3b87c856397462713c",
            "0891e7c64e814af0bfa1b9a82a91018e",
            "f9fbc4c5c0c946adab29144c4cbc147b",
            "12c83d7650d24f468fc439581f9256e5",
            "afb06e490e364d09ad232f240cc2d711",
            "58eb0f9f01bd4f88bf3c883547a6226d"
          ]
        },
        "id": "kVOZY5E0uvD3",
        "outputId": "bc430a94-77af-41ac-c469-814fbf854a95"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e1ca2a6c9dc46e68c0b8da956c08294",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2e2145d289d4c7fac22d4ca73e0831f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc10008f48444b5db516d07b5aa07931",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Model for LoRA\n",
        "- Use `prepare_model_for_kbit_training` to make the quantized model trainable.\n",
        "- This step adjusts layer norms and gradients for low-bit training.\n"
      ],
      "metadata": {
        "id": "JY8A9sIaSas2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTeRBLfeuwXb"
      },
      "outputs": [],
      "source": [
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define LoRA Configuration\n",
        "- Create a `LoraConfig` specifying target modules and LoRA hyperparameters.\n",
        "- Used to inject LoRA adapters into the base model for efficient fine-tuning.\n"
      ],
      "metadata": {
        "id": "tBwRlvScSc3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kOZHlFpuxk_"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaiAryeduyDg",
        "outputId": "2c8a85fb-6951-41ce-bf0e-12045abb15b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset\n",
        "- Load training data using Hugging Face `datasets`.\n",
        "- This dataset will be tokenized and formatted for fine-tuning.\n"
      ],
      "metadata": {
        "id": "eWRCppJESfYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8RIPn1Bu138"
      },
      "outputs": [],
      "source": [
        "# Function to extend position embeddings\n",
        "def extend_position_embeddings(model, target_length):\n",
        "    \"\"\"\n",
        "    Extend the position embeddings of a model to support longer sequences.\n",
        "    Uses linear interpolation to create new position embeddings.\n",
        "    \"\"\"\n",
        "    print(f\"Extending position embeddings from {ORIGINAL_CTX_LENGTH} to {target_length}\")\n",
        "\n",
        "    # For models with rotary embeddings, like TinyLlama\n",
        "    if hasattr(model, \"config\") and hasattr(model.config, \"max_position_embeddings\"):\n",
        "        model.config.max_position_embeddings = target_length\n",
        "\n",
        "    if hasattr(model, \"config\") and hasattr(model.config, \"max_sequence_length\"):\n",
        "        model.config.max_sequence_length = target_length\n",
        "\n",
        "    # Update model internals for RoPE\n",
        "    # Accessing the layers using _modules instead of model.layers\n",
        "    for name, layer in model.base_model._modules.items():  # Assuming base_model holds the original model\n",
        "        if name.startswith(\"layers\"): # The layers might have names like 'layers.0', 'layers.1', etc.\n",
        "            if hasattr(layer.self_attn, \"rotary_emb\"):\n",
        "                layer.self_attn.rotary_emb.max_seq_len = target_length\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kWnTh79u3sk",
        "outputId": "8d4e6238-01f4-4e0b-b0eb-093a2eb46822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extending position embeddings from 2048 to 8192\n"
          ]
        }
      ],
      "source": [
        "# Extend model's position embeddings\n",
        "model = extend_position_embeddings(model, TARGET_CTX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "65b83686caac4fc19244fedcd2423cd0",
            "44663420836849f5a80770cb99ccb793",
            "203709f9a8d14d02b5ebf28f4077f057",
            "afd7a60bec274e078f622dd6f5fbadff",
            "2fb0614c1c3641059a7e09ee1bc57108",
            "4699c399548f444b98b9d6326f704114",
            "87e526f701c847cfaf1691f5a18d365e",
            "b575a16af9244ffca8e98c86e3df589f",
            "896e78dc80b24303b3f924cb82c5a092",
            "8ccd446bec1448ce889117047fd93a4d",
            "046a69c578874099a932c602fe90e829",
            "a1db8a59ebd04ed99ebc03cf1ff3e6f2",
            "70bea7b9e4944d859e029fb8e9852e1e",
            "b9176aeb95554f2db40e1654cc51126f",
            "69d6801fa6b449d0a805eebc9d612026",
            "f970ab1b6e204f5689a617582ab93c39",
            "9b809ddbcd0143579d3b089a36c0fa5e",
            "fde608894c3f4c18a89c0172328e0dd0",
            "56ba9e8732a846518075008e86e5a3e6",
            "2c7943a840df4b008201dded99bf0b35",
            "eae4166c77324da8a0e3bd5fc60b3f79",
            "fb9612b30e9647d6b13c805273a992ba",
            "4b3be7a464b04d21808d3226ab140a22",
            "f671f47943d740819363a74d59a78988",
            "a456853fcb51474ca7bc91ff70e07793",
            "293e087855d34b3b98e92f07cb7e838b",
            "162ef6ebed8a4d92b2808f808dc83707",
            "08c260dec36746bf8e0ddb1f2bfbca7b",
            "56c5b25c3b134cf2b76ce569cc2c39bb",
            "bd17ae4b7fb14cd2b61abe9f95d92bc1",
            "ee77aa4331c54f59a33babdad3ec277d",
            "7fc16addb1894cc583de445e2d927a87",
            "08304ef56d764d7c92f4db72f68f5a96",
            "fdaf5a00931a48bcbf4864fe1ae32137",
            "1b81c410a1e94e59a2237577fe16ae5a",
            "ef6852e6c99a4105b8a4c7409157d104",
            "e3413e4a576148d795a381ec65891749",
            "033f7d55905447c9a7390b59b40f7559",
            "686f431cd6b144ebbadba20e5b8042bd",
            "fde9b4f73981461a94b10b30784eb0a4",
            "bbc84dee7a0348928cf92e97b91dc67f",
            "38ad3ba06eb74722b928bb0720846932",
            "2cbe8dfa180d4082b444735a8f9f7e8e",
            "216a9cc60193460db57bbe3048cbcfeb",
            "f62d17983fa54ca293a36ce44f4a025b",
            "99b25de469134edaae815d714481b80c",
            "9713a876b76647edb8e6978abd83c88f",
            "77f83a2a815e4bb0a8255c5413450e7c",
            "a4a1080fba394dadad60339bb90787b2",
            "a0b7a0a88bc94322bdd8854fe37af5be",
            "1b02baf723984e97a5252254d0db8718",
            "05ee4051f4b843d19bf9991268346e40",
            "e85cbee6d07e48acb1d1fd6165cc5187",
            "843f1da7965b45bcad143101c578e67b",
            "3def3c89fa9e48bfb674c36fae160ea1",
            "0ae0066df7064715aacd78a9836cb660",
            "30f2cbf911f14834aaa67f9c0851cabf",
            "91fde4edcd374504b03d70446395296c",
            "69e910f3e11c4d00ba23055f5db8b99b",
            "fd5a9b067f214fbd9623c9592969b8b1",
            "c1430beb166b4394b7dca56208b98b33",
            "40f5c44fb1e6400c9552857e5925ecca",
            "88ae2659aad5473291023dc35e5248a9",
            "f0480bfd38684f1ea7384beae0e5766f",
            "5e60883fb87d4b9d9c06b06d010f239c",
            "902c890d87694a10aef225bcd2d6323e",
            "882eb4ae8ce34cb5bb81b5a6079955a6",
            "cbe334e82c36469b9f0ff5bd1494b059",
            "ce6c21981c804c1098943e551e51ff8c",
            "0edde29e5a314e3daba1be6b30ad5fe1",
            "7555ccb899404117b9b147b424c1ee5d",
            "220266b7a63c4138b961bf1daabc2a47",
            "d76218709b2a4d8bbe39d83ab9396a8c",
            "170afada3bd34f76bdc517c17fc75979",
            "18ee3329922f4c7f97a6b668a6fe4237",
            "5c156e490cf34b4f8883084fbd6450a0",
            "984e371a894743bbba5243188e712e11",
            "82f5e5e3e202493eb55002d8c5a748ee",
            "017ad414ce4c43f396ab85a59d5b96b5",
            "3aba151dfbb2410dbf9121309fe2a5e8",
            "35b7598775e747b5a690514664bb47df",
            "806caef82b0f45b590e9ddba91faeaa0",
            "8572ff16d9f04be5b5b8dc59d60fe0bc",
            "559bd2263b584501be8beb4c48929f48",
            "4cc1b8ce7537421eabb932b15d8fcd91",
            "7a8966881dad430987d235c638a3d479",
            "c9cf7c7901ae45b29e2c5025fc929f3b",
            "df8706902e99457ba1136384c228dbc6"
          ]
        },
        "id": "uPtqOVg6u5fp",
        "outputId": "9f42af40-53d7-4882-f86c-db2b8e7d05ee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65b83686caac4fc19244fedcd2423cd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1db8a59ebd04ed99ebc03cf1ff3e6f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/722k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b3be7a464b04d21808d3226ab140a22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdaf5a00931a48bcbf4864fe1ae32137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f62d17983fa54ca293a36ce44f4a025b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ae0066df7064715aacd78a9836cb660",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "882eb4ae8ce34cb5bb81b5a6079955a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82f5e5e3e202493eb55002d8c5a748ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Wikitext which is much smaller\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtXaMz5Pu7bj"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the examples\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=TARGET_CTX_LENGTH,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "\n",
        "    # Filter out examples that are too short (< 1024 tokens)\n",
        "    long_enough = [length >= 1024 for length in tokenized_examples[\"length\"]]\n",
        "\n",
        "    result = {\n",
        "        \"input_ids\": [ids for ids, is_long in zip(tokenized_examples[\"input_ids\"], long_enough) if is_long],\n",
        "        \"attention_mask\": [mask for mask, is_long in zip(tokenized_examples[\"attention_mask\"], long_enough) if is_long]\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d68d84b5f8834a95bce1d3e190a9f7df",
            "54f13e36681d45c39a00cc7563e19a72",
            "be1ac589eda74968ab371113de468d4b",
            "ba659f364d6542b8b5645d82a88ca081",
            "d89565ca39144a3d950d4cb3b39008e0",
            "65af2f1a1f30474cbed6dcf2e37200e5",
            "009fa471989244b1aa88cb49a043cccf",
            "671250530b31413c9d91118b4cbfe17c",
            "29c3a01a61eb42bf9b53efd9ba93f674",
            "e1db8ef843194395ae12afd170a9b7c0",
            "b862c6838bad4dffb5c861d83b247f76"
          ]
        },
        "id": "jdYVN-Tbu7Ys",
        "outputId": "de5586ca-519d-4676-9cae-c48bc09af059"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d68d84b5f8834a95bce1d3e190a9f7df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Preprocessing dataset (num_proc=4):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset size: 1801350\n",
            "Processed dataset size: 21\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "processed_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    num_proc=4,\n",
        "    desc=\"Preprocessing dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Original dataset size: {len(dataset)}\")\n",
        "print(f\"Processed dataset size: {len(processed_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqTDhyVpu7WK",
        "outputId": "b0b97c90-3f84-481c-99be-952629781dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 18\n",
            "Eval dataset size: 3\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset\n",
        "train_dataset = processed_dataset.shuffle(seed=42).select(range(int(0.9 * len(processed_dataset))))\n",
        "eval_dataset = processed_dataset.shuffle(seed=42).select(range(int(0.9 * len(processed_dataset)), len(processed_dataset)))\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-_c4IoZvA2L",
        "outputId": "72291ff7-d723-4654-8f85-7e2dfe799f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.51.1\n"
          ]
        }
      ],
      "source": [
        "# Configure training arguments (compatible with older Transformers versions)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/tinyllama-context-extension\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    # Replace evaluation_strategy\n",
        "    eval_steps=100,          # Will be ignored if eval_strategy not set\n",
        "    logging_steps=10,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    save_steps=500,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    # Remove report_to=\"wandb\" if not installed\n",
        "    run_name=\"tinyllama-context-extension\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# Alternative approach - first check which version of transformers you have\n",
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# For very old versions, use a minimal config\n",
        "if transformers.__version__ < \"4.0.0\":\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results/tinyllama-context-extension\",\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        save_steps=500,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdlRpRIPvBgu"
      },
      "outputs": [],
      "source": [
        "# Create data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd8ytZrpvBeP",
        "outputId": "2c6390ff-707b-4394-c5cf-ac133be69ad2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "I5mFIApyvBbn",
        "outputId": "94dcdf93-aa4d-40c5-dd83-c50365d0fc9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyedanida-khader\u001b[0m (\u001b[33msyedanida-khader-san-jose-state-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250415_202132-bm8hlv4m</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/syedanida-khader-san-jose-state-university/huggingface/runs/bm8hlv4m' target=\"_blank\">tinyllama-context-extension</a></strong> to <a href='https://wandb.ai/syedanida-khader-san-jose-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/syedanida-khader-san-jose-state-university/huggingface' target=\"_blank\">https://wandb.ai/syedanida-khader-san-jose-state-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/syedanida-khader-san-jose-state-university/huggingface/runs/bm8hlv4m' target=\"_blank\">https://wandb.ai/syedanida-khader-san-jose-state-university/huggingface/runs/bm8hlv4m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 01:38, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=2.3735100428263345, metrics={'train_runtime': 299.3453, 'train_samples_per_second': 0.18, 'train_steps_per_second': 0.01, 'total_flos': 291156052451328.0, 'train_loss': 2.3735100428263345, 'epoch': 1.8})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SnSeVw8vEwg",
        "outputId": "16b98274-a6e3-4160-9702-fabb1ab47766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./final_model\")\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWwLHW2VvEtu",
        "outputId": "dda71cdd-0554-438a-b1f9-4f64c1ed19b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with extended context...\n"
          ]
        }
      ],
      "source": [
        "# Evaluation and demonstration section\n",
        "print(\"Evaluating the model with extended context...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnuOeY4LvErD"
      },
      "outputs": [],
      "source": [
        "# Load the trained model for evaluation without device_map\n",
        "from peft import PeftModelForCausalLM # Import PeftModelForCausalLM from peft\n",
        "\n",
        "# Load the base model first\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID, # Assuming MODEL_ID is defined and refers to the base model\n",
        "    quantization_config=bnb_config, # If quantization was used\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "# Then load the PEFT weights\n",
        "trained_model = PeftModelForCausalLM.from_pretrained(\n",
        "    base_model, # Pass the base model instance\n",
        "    \"./final_model\", # Path to the PEFT weights\n",
        ")\n",
        "\n",
        "\n",
        "# Move the model to the desired device if needed\n",
        "if torch.cuda.is_available():\n",
        "    trained_model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJX9LtZlvNVK"
      },
      "outputs": [],
      "source": [
        "# Test context window capability\n",
        "def test_context_window(model, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Test how the model handles a long context window by feeding it\n",
        "    a prompt and checking for coherent completion.\n",
        "    \"\"\"\n",
        "    # Create a shorter text input\n",
        "    long_text = \"\"\n",
        "    for i in range(50): # Reduced from 200 to 50\n",
        "        if i % 10 == 0:\n",
        "            long_text += f\"\\n\\n==== SECTION {i//10 + 1} ====\\n\\n\"\n",
        "        long_text += f\"This is paragraph {i+1} in our test of the extended context window. \"\n",
        "        long_text += f\"If the model can see this far, it should remember we are in section {i//10 + 1}. \"\n",
        "\n",
        "    # Add a question at the end\n",
        "    prompt = long_text + \"\\n\\nQuestion: Which section number did we start with? Answer: \"\n",
        "\n",
        "    # Tokenize and check length\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    input_length = tokens.input_ids.shape[1]\n",
        "    print(f\"Input length: {input_length} tokens\")\n",
        "\n",
        "    if input_length > max_length:\n",
        "        print(f\"Input exceeds maximum length of {max_length}, truncating...\")\n",
        "        tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **tokens,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    # Decode and print the result\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Model output:\\n{decoded_output[len(prompt):]}\")\n",
        "    return input_length, decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO6KNVK0vNSi",
        "outputId": "aa7933a7-7e17-4993-84ff-4024ece3aae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing with different context lengths:\n",
            "\n",
            "==== Testing with approximately 1000 tokens ====\n",
            "Input length: 1718 tokens\n",
            "Model output:\n",
            "1.\n",
            "\n",
            "==== Testing with approximately 4000 tokens ====\n",
            "Input length: 1718 tokens\n",
            "Model output:\n",
            "1\n",
            "\n",
            "==== Testing with approximately 7000 tokens ====\n",
            "Input length: 1718 tokens\n",
            "Model output:\n",
            "1\n",
            "\n",
            "Memory usage analysis:\n",
            "Original context length: 2048\n",
            "Extended context length: 8192\n",
            "Ratio: 4.0x\n"
          ]
        }
      ],
      "source": [
        "# Test with different context lengths\n",
        "print(\"\\nTesting with different context lengths:\")\n",
        "for test_length in [1000, 4000, 7000]:\n",
        "    print(f\"\\n==== Testing with approximately {test_length} tokens ====\")\n",
        "    actual_length, _ = test_context_window(trained_model, tokenizer, TARGET_CTX_LENGTH)\n",
        "\n",
        "# Memory usage analysis\n",
        "print(\"\\nMemory usage analysis:\")\n",
        "print(f\"Original context length: {ORIGINAL_CTX_LENGTH}\")\n",
        "print(f\"Extended context length: {TARGET_CTX_LENGTH}\")\n",
        "print(f\"Ratio: {TARGET_CTX_LENGTH/ORIGINAL_CTX_LENGTH}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMR46MScvNPw",
        "outputId": "0da02093-163c-442f-a952-63164d02a4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Visualizing attention patterns:\n",
            "Short text: 7 tokens\n",
            "Medium text: 512 tokens\n",
            "Long text: 512 tokens\n"
          ]
        }
      ],
      "source": [
        "# Visualize context handling (with sequence lengths)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot attention patterns (visualization code)\n",
        "def visualize_attention(model, tokenizer, text, layer_idx=0, head_idx=0):\n",
        "    \"\"\"\n",
        "    Visualize the attention pattern for a given input at a specific layer and head.\n",
        "    \"\"\"\n",
        "    # Reduce max_length in tokenizer call\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
        "    input_length = tokens.input_ids.shape[1]\n",
        "\n",
        "    # Forward pass to get attention\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens, output_attentions=True)\n",
        "\n",
        "    # Get attention for the specified layer and head\n",
        "    # Convert attention weights to float32 before converting to NumPy array\n",
        "    attention = outputs.attentions[layer_idx][0, head_idx].type(torch.float32).cpu().numpy()\n",
        "\n",
        "    # Plot the attention pattern\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(attention, cmap='viridis')\n",
        "    plt.title(f\"Attention Pattern - Layer {layer_idx}, Head {head_idx}\")\n",
        "    plt.xlabel(\"Key Sequence Position\")\n",
        "    plt.ylabel(\"Query Sequence Position\")\n",
        "    plt.colorbar(label=\"Attention Weight\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"attention_l{layer_idx}_h{head_idx}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return input_length\n",
        "\n",
        "# Generate a paragraph of text at different lengths for visualization\n",
        "short_text = \"This is a short test.\"\n",
        "medium_text = \" \".join([\"This is paragraph \" + str(i) for i in range(100)])\n",
        "long_text = \" \".join([\"This is paragraph \" + str(i) for i in range(500)])\n",
        "\n",
        "print(\"\\nVisualizing attention patterns:\")\n",
        "for text, name in [(short_text, \"short\"), (medium_text, \"medium\"), (long_text, \"long\")]:\n",
        "    length = visualize_attention(trained_model, tokenizer, text)\n",
        "    print(f\"{name.capitalize()} text: {length} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtVwOBuxvURW",
        "outputId": "3b6a36a3-dd6d-4895-d239-7cf543fd4641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Measuring performance metrics (safely):\n",
            "Available GPU memory: 15.83GB\n",
            "Current memory usage: 11.13GB\n",
            "Processing sequence of length 7, estimated memory: 0.00GB\n",
            "Input length: 7, Inference time: 3.4475s, Memory used: 0.0000GB\n",
            "Processing sequence of length 241, estimated memory: 0.00GB\n",
            "Input length: 241, Inference time: 2.4050s, Memory used: 0.0000GB\n",
            "Processing sequence of length 1091, estimated memory: 0.01GB\n",
            "Input length: 1091, Inference time: 1.5944s, Memory used: 0.0000GB\n",
            "Processing sequence of length 2891, estimated memory: 0.07GB\n",
            "Input length: 2891, Inference time: 4.1950s, Memory used: 0.0000GB\n"
          ]
        }
      ],
      "source": [
        "# Modified performance comparison function with memory protection\n",
        "def measure_performance_safe(model, tokenizer, inputs):\n",
        "    \"\"\"\n",
        "    Measure inference time and memory usage for different input lengths\n",
        "    with protection against OOM errors.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in inputs:\n",
        "        try:\n",
        "            # Tokenize\n",
        "            tokens = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "            input_length = tokens.input_ids.shape[1]\n",
        "\n",
        "            # If the input is too long, truncate it\n",
        "            if input_length > TARGET_CTX_LENGTH:\n",
        "                tokens = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
        "                                  max_length=TARGET_CTX_LENGTH).to(model.device)\n",
        "                input_length = tokens.input_ids.shape[1]\n",
        "\n",
        "            # Print expected memory requirement (rough estimate)\n",
        "            estimated_memory = (input_length**2) * 4 * 2 / 1e9  # Very rough estimate in GB\n",
        "            print(f\"Processing sequence of length {input_length}, estimated memory: {estimated_memory:.2f}GB\")\n",
        "\n",
        "            # Free up GPU memory before measurement\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Measure GPU memory before\n",
        "            torch.cuda.synchronize()\n",
        "            mem_before = torch.cuda.memory_allocated() / 1e9  # GB\n",
        "\n",
        "            # Measure inference time\n",
        "            start_time = torch.cuda.Event(enable_timing=True)\n",
        "            end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            # Use smaller max_new_tokens for longer sequences\n",
        "            max_tokens = 20 if input_length < 1000 else 5\n",
        "\n",
        "            start_time.record()\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **tokens,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    # Use more memory-efficient generation settings\n",
        "                    use_cache=True,\n",
        "                    do_sample=False  # Deterministic generation uses less memory\n",
        "                )\n",
        "            end_time.record()\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            inference_time = start_time.elapsed_time(end_time) / 1000  # seconds\n",
        "\n",
        "            # Measure GPU memory after\n",
        "            mem_after = torch.cuda.memory_allocated() / 1e9  # GB\n",
        "            mem_used = mem_after - mem_before\n",
        "\n",
        "            results.append({\n",
        "                \"length\": input_length,\n",
        "                \"time\": inference_time,\n",
        "                \"memory\": mem_used\n",
        "            })\n",
        "\n",
        "            print(f\"Input length: {input_length}, Inference time: {inference_time:.4f}s, Memory used: {mem_used:.4f}GB\")\n",
        "\n",
        "            # Free memory after each run\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"Skipping sequence length {input_length} due to OOM error\")\n",
        "                # Free memory after error\n",
        "                torch.cuda.empty_cache()\n",
        "                # Add partial result\n",
        "                results.append({\n",
        "                    \"length\": input_length,\n",
        "                    \"time\": float('nan'),\n",
        "                    \"memory\": float('nan')\n",
        "                })\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "        # Add a small delay between tests to help memory recovery\n",
        "        import time\n",
        "        time.sleep(2)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Use smaller and fewer test inputs to avoid OOM\n",
        "test_inputs = [\n",
        "    \"This is a short test.\",\n",
        "    \" \".join([\"Sentence \" + str(i) for i in range(50)]),\n",
        "    \" \".join([\"Sentence \" + str(i) for i in range(200)]),\n",
        "    \" \".join([\"Sentence \" + str(i) for i in range(500)])  # Max ~2000 tokens\n",
        "]\n",
        "\n",
        "print(\"\\nMeasuring performance metrics (safely):\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB\")\n",
        "    print(f\"Current memory usage: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
        "\n",
        "    # Force garbage collection\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    perf_results = measure_performance_safe(trained_model, tokenizer, test_inputs)\n",
        "\n",
        "    # Plot results (skip NaN values if any)\n",
        "    valid_results = [(r[\"length\"], r[\"time\"], r[\"memory\"])\n",
        "                     for r in perf_results\n",
        "                     if not (np.isnan(r[\"time\"]) or np.isnan(r[\"memory\"]))]\n",
        "\n",
        "    if valid_results:\n",
        "        lengths, times, memories = zip(*valid_results)\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(lengths, times, 'o-')\n",
        "        plt.title(\"Inference Time vs. Sequence Length\")\n",
        "        plt.xlabel(\"Sequence Length (tokens)\")\n",
        "        plt.ylabel(\"Time (seconds)\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(lengths, memories, 'o-')\n",
        "        plt.title(\"Memory Usage vs. Sequence Length\")\n",
        "        plt.xlabel(\"Sequence Length (tokens)\")\n",
        "        plt.ylabel(\"Memory (GB)\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"performance_metrics.png\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"No valid performance measurements to plot\")\n",
        "else:\n",
        "    print(\"CUDA not available, skipping performance measurements\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}