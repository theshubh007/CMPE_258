# Assignment 9: Transformers and NLP

## Overview

This repository demonstrates practical implementations of **Transformer-based NLP models** using **Keras NLP** and **Hugging Face**. The assignment explores various aspects of modern NLP techniques from basic inference with pre-trained models to building transformers from scratch.

---

## Objective

The goal of this assignment is to demonstrate practical implementations of key transformer and NLP techniques by:

- Using pre-trained models for inference tasks
- Fine-tuning transformer models for specific NLP tasks
- Building and training a transformer architecture from scratch
- Documenting the process and results with detailed explanations
- Providing a video walkthrough with debugging traces

---
## Walkthrough Video

[ðŸŽ¥ Watch Assignment 9 Demo Walkthrough](https://youtu.be/eIKQMEC_8Cg)

- Full Colab walkthrough
- Debugging traces and visualizations
- Performance analysis and comparisons

---
## Included Notebooks

This assignment contains three fully annotated and modular notebooks:

- `PartA_Inference_PreTrained_Classifier.ipynb`: Demonstrates inference with pre-trained NLP models
  - Text classification
  - Sentiment analysis
  - Text generation
  - [âœ… Google Colab](https://colab.research.google.com/drive/1cLfnji-YTmgQxpMP2mZxPxCK2Kq6eAyW?usp=sharing)

- `PartB_Finetune_Transformer.ipynb`: Shows how to fine-tune pre-trained transformer models
  - Transfer learning with BERT
  - Domain adaptation
  - Performance evaluation
  - [âœ… Google Colab](https://colab.research.google.com/drive/1zo0fENauhdrTfp3k74L_eWHlfADstucc?usp=sharing)

- `PartC_Build_Transformer_From_Scratch.ipynb`: Builds a transformer architecture from scratch
  - Attention mechanism implementation
  - Positional encoding
  - Multi-head attention
  - Training and evaluation
  - [âœ… Google Colab](https://colab.research.google.com/drive/15eezcNnQNPKE19tx4B4cmWLO5EqrKhO0?usp=sharing)

Each Colab includes markdown explanations, execution logs, visualizations, and detailed analysis of results.

---

## NLP Techniques Demonstrated

- **Text Classification** â€“ Using transformers for categorizing text
- **Sentiment Analysis** â€“ Determining emotional tone in text
- **Text Generation** â€“ Creating coherent text with language models
- **Transfer Learning** â€“ Adapting pre-trained models to new tasks
- **Architecture Design** â€“ Building transformer components from scratch

---

## Frameworks & APIs Used

### Keras NLP

- High-level API for NLP tasks
- Pre-trained models and tokenizers
- End-to-end transformer implementations

[Keras NLP Documentation](https://keras.io/keras_nlp/)

### Hugging Face Transformers

- State-of-the-art transformer models
- Easy model loading and fine-tuning
- Comprehensive model hub

[Hugging Face Documentation](https://huggingface.co/docs)

---



> **Note**: Update this link with your actual YouTube video URL once uploaded.

---

## Implementation Details

### Part A: Inference with Pre-trained Models
- Uses Keras NLP and Hugging Face for text classification tasks
- Demonstrates zero-shot and few-shot inference capabilities
- Shows how to use pre-trained models for text generation
- Compares performance across different model architectures

### Part B: Fine-tuning Transformer Models
- Adapts BERT-based models to specific downstream tasks
- Implements efficient fine-tuning strategies
- Demonstrates techniques to prevent catastrophic forgetting
- Evaluates performance improvements from fine-tuning

### Part C: Building Transformers from Scratch
- Implements the core transformer architecture components
- Builds attention mechanisms and positional encodings
- Trains the model on text classification tasks
- Analyzes the learning process and model behavior

---

## References

- [Keras NLP Examples](https://keras.io/examples/nlp)
- [Keras Hub Transformer Pre-training](https://keras.io/keras_hub/guides/transformer_pretraining)
- [Keras NLP Guides](https://keras.io/keras_nlp/#guides)
- [Google I/O 2023 NLP Tutorial](https://io.google/2023/program/79e77594-3e72-4df2-a754-916af4f29ba9)
- [Hands-On Large Language Models - Chapter 11](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter11/Chapter%2011%20-%20Fine-Tuning%20BERT.ipynb)

---